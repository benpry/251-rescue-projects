---
title: "TODO (re-replication) (student) (cumulative science) (try again) (replication failure) "
author:
  - name: Veronica Boyce
    affiliation: Stanford
    footnote:
      - corresp
  - name: TODO
    affiliation: Stanford
  - name: Michael C. Frank
    affiliation: Stanford
address:
  - code: Stanford
    address: Stanford University
footnote:
  - code: corresp
    text: "Corresponding author. Email: vboyce@stanford.edu"
bibliography: ["251rescue.bib"] # Replace with one or more of your own bibtex files. Better BibTeX for Zotero is your friend
csl: apa6.csl # Use any CSL style. See https://www.zotero.org/styles for a good list. Ignored if citation_package: natbib
link-citations: TRUE
output:
  bookdown::pdf_document2:
    toc: FALSE
    keep_tex: TRUE
    template: generic_article_template.tex
    #md_extensions: "-autolink_bare_uris"
    number_sections: TRUE
    citation_package: default # Can also be "natbib"
lang: en # Main document language in BCP47 format
geometry: "margin=25mm"
papersize: a4
#linestretch: 2 # for double spacing
endfloat: FALSE # Set to TRUE to turn on latex endfloat package to place figures and tables at end of document
# endfloatoption: # See endfloat documentation for more possibilities
#   - tablesfirst # Default
#   - nomarkers # Default
numberlines: FALSE
authblk: TRUE # FALSE = author affiliations in footnotes; TRUE = author affiliations in a block below author names
footnotehyper: FALSE # TRUE will give you enhanced table footnote capabilities. Set to FALSE to be able to use French blocks. Needed due to what appears to be a latex bug.
urlcolor: blue
linkcolor: blue
citecolor: blue
graphics: TRUE # Needed to be able to include images
tables: TRUE # Needed to be able to include tables
# fancyhdr:
#   first:
#     #headleft: "REPORT-NO-XXXX"
#     headright: "Kaplan et al. (2021)"
#     headrulewidth: 0pt
#     #footleft: A left foot
#     footrulewidth: 0pt
#   subsequent:
#     #headleft: "NEXT-PAGE-HEADER-LEFT"
#     headright: "Kaplan et al. (2021)"
#     headrulewidth: 1pt
#     footrulewidth: 0pt

header-includes:
 - \usepackage{tikz}
 - \usetikzlibrary{positioning,chains}
 - \usepackage{setspace}\singlespacing
 - \renewcommand{\textfraction}{0.00}
 - \renewcommand{\topfraction}{1}
 - \renewcommand{\bottomfraction}{1}
 - \renewcommand{\floatpagefraction}{1}
 - \setcounter{topnumber}{3}
 - \setcounter{bottomnumber}{3}
 - \setcounter{totalnumber}{4}

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE) # By default, hide code; set to TRUE to see code
#knitr::opts_chunk$set(fig.pos = 'th') # Places figures at top or here
knitr::opts_chunk$set(out.width = '100%', dpi=300,
                      fig.width=8, fig.width=8) # Figure resolution and size
knitr::opts_chunk$set(fig.env="figure") # Latex figure environment

options(knitr.table.format="latex") # For kable tables to work without setting format option

knitr::opts_chunk$set(echo=F, warning=F, message=F)#dev = "png", dev.args = list(type = "cairo-png")
 library("papaja")
library("bookdown")
library("rticles")
 library(here)
 library(tidyverse)
 library(brms)
library(tidybayes)
library(kableExtra)
library(viridis)
library(cowplot)
library(ggthemes)
library(reshape2)
library(knitr)
library(kableExtra)
library(metafor)
#r_refs("r-references.bib", append=F)
theme_set(theme_bw())
options(knitr.table.format = "pdf")

model_location="code/models"

```

<!---------------------- Abstract --------------------->

::: {.abstract data-latex="" lang=en}
Title ideas:

Can failed replications be rescued? Evidence from N re-replication experiments suggests rarely.

Try again? Attempts to rescue failed replications were generally unsuccessful

A low rate of successfully replicating experiments with a prior unsuccessful replication

Once is probably enough. Low replication rates when re-replicating N studies with failed replications. 

* may want to include "student" or "class"


Abstract:

In order to do cumulative science, results need to be replicable. However, an individual direct replication could get a different result than an original ("replication failure") for a number of reasons, which vary in whether they would apply to a potential second replication. Given one failed replication, how likely is a second attempt at replication to succeed? As a pragmatic matter, when a replication fails, scientists have to decide: do they make a second attempt at replication, or move on? 

We re-replicated 17 experiments that had previously had a failed student replication, reported in Boyce et al. (2023). In N/17 of these rescue projects (TODO %), the results mostly or fully replicated. We discuss the relative effect sizes and levels of statistical consistency between original effect sizes and replication effect sizes. 

While a few studies with failed replications were rescued with larger sample sizes and experimental tweaks, our overall results indicate that most failed replications stayed failed. If the goal is to do cumulative science, one may wish to stop after one failed replication. 

TODO MAKE THIS ABSTRACT BETTER!

:::

# Introduction

Imagine the scenario where you are a new graduate student, and you run a replication of a study in the literature that you are interested in building upon in your research. The replication fails. Should you try again, running another replication, perhaps with a larger sample or other tweaks to possibly fix the replication? Or should you give up on replicating that specific study, and find a different study to replicate and build upon? 

There has been much discussion in the literature about how we should interpret replication results in terms of the existence of a true effect or effect size. TODO CITATIONS

<!--Well, it depends on what you're trying to measure. From the perspective of learning from the field as a whole things are complicated and may depend also on a priori information from other related studies, the beleivability of the results, the relative rigor and sample sizes of the original and replication (TODO perhaps cite the time invariant). -->
We take a different angle on replications, and instead focus scientists decisions on what studies to run. Individual studies (and replications) are rarely definitive when it comes to scientific evidence, but they often influence what studies a scientist conducts.  If a study works, scientists are likely to continue to purse that result, but if a study doesn't work, scientists have to choose whether to try again, or switch to a different study. 

How definitive is a replication failure? That is, if a study fails to replicate, what are the chances that an additional replication succeeds. After CITE OSF found an overall replication rate of WHATEVER, concerns were raised by @gilbert that method differences were the problem. In 11 of the OSF studies, there had been concerns about protocol fidelity raised by original authors prior to data-collection. 10 out of these 11 failed to obtain significant results in the replication. In a follow-up, @ebersole2020 re-replicated 10 of these 11 in large samples, using both the RP:P protocol and a new protocol revised under advise of the original authors or other experts. None of the RP:P protocols found significant results, 2 of the 10 revised did (but not the 1 that was significant in the original RP:P sample). The replication effect size was much smaller CITE NUMBER. We could frame @ebersole2020 as an attempt to rescue the original effects of the 9 studies with significant original effects and non-significant RP:P effects by a combination of high power and design tweaks. 


## Why replication failure? 
The idea of replication failure implies a certain time direction: first a "successful" study and (later) a "failed" replication. (CITE TIME INVARIANCE) We can also consider this as two very-related studies that got different results, and thus the question is what difference between these two studies could account for the differing results. 

[not sure if it would be better to structure this differently]

A replication can have different results from the original study for a number of different potential reasons (see also @ebersole2020). The original result may have been a fluke, through chance or p-hacking. The original result might be of much less generalizability than is assumed, and very sensitive to the exact conditions and time it was run in ("hidden moderators"). The original could have an inflated effect size, due to some combination of heterogeneity, p-hacking, luck, and publication bias. 

The replication could have been underpowered due to any of inflated original effect size, attrition in sample, or the original also being underpowered. A manipulation might have failed in the replication (this is a proximate cause that also raises the question of why the manipulation check failed). There might have been a difference in materials or instructions between the two, that turns out to matter (more likely if materials are not available). The experiment might not port well to online due to some details. A change in sample (ie. to online) may decrease data quality without sufficient attention checks. The replication might have made changes (for convenience or a new sample) that turn out to matter. 

[ maybe try to make the point here that if there are changes in time/place/population, you want a good *translation* of the experiment not an identical copy]

While one can theorize after the fact about potential causes, in most circumstances it is hard to diagnose for certain what made for the difference in the results. An exception might be if a specific error in one study or the other is found, such that the results were not what was thought (analytic error) or the data was not what was thought (experimental error). 

## Re-replication
What would these potential causes of replication failure suggest would happen in another replication? For our purposes, we're considering another replication with roughly the same environment, knowledge, and resources of the first replication, but with the benefit of hindsight on the first replication. 

Heterogeneity and lack-of-generalizability reasons mostly suggest another failed replication, as the second replication is likely to be closer to the first replication than to the original in these random factors. CITE THAT PAPER about types of heterogeneity and which way they would fall

If the original result was p-hacked or a luck false positive, then another replication is also likely to fail.

Manipulation check failure may not be fixable, but it might at least be diagnosable. 

Underpoweredness and inflated effect size may be recoverable depending on the extent -- if the effect size was inflated and the first replication powered for the effect, a better powered (but not huge) replication may be able to recover a smaller, but still convincing effect.

Changes between original and replication may or may not be fixable -- if the original materials are still unavailable, a second replication is no better off. If a second replication can get closer to the original, either by using closer materials, or better adapting the study to a new environment, the rescue might succeed at better approximating the results of the original.

Sensitivity to environment or not working well online are unlikely to be fixable if the replications remain constrained to using online methods. Similarly, if the result is specific to a special population, then a new replication is also likely to fail, unless it has gained access to this population.  

<!--These are pragmatic judgments related to how a lab might treat a failed replication as a sign that they cannot do cumulative science on a particular paradigm. In some cases, especially related to changes-in-population or changes-to-online, replications, and thus cumulative work on the paradigm, might be possible for other groups who have different resources. 

The question is, having a failed replication, and not knowing which reasons might have caused the different results -- should a scientist try again? -->

Overall, it seems that there are both potential causes of replication failure that may be resolvable even with a similar level of resources and potential causes that are not resolvable, either at all, or given resource constraints. 

## Current study

```{r}


library(tidyverse)
library(testthat)
all_projects <- read_csv("https://raw.githubusercontent.com/vboyce/251-254-MA/main/data/raw_data.csv")

filtered <- all_projects |>
  mutate(sub_rep=ifelse(replicated_instructor_code==replicated_report_code, 
                 replicated_report_code, 
                 adjudicated_replication_code)) |> 
  filter(sub_rep %in% c(0,.25, .5)) |> 
  filter(include!="no") |> 
  filter(target_N <=200) |> 
  filter(academic_year %in% c("2015-2016", "2016-2017", "2017-2018", "2018-2019", 
                              "2019-2020", "2020-2021","2021-2022", "2022-2023")) 
# what we really want is whether there was a github, but that column is private and so not in the public raw data
# but 2015 is the first year where we have github links

#test_that("number of rows in filtered", {expect_equal(nrow(filtered),49)})

#given this sample, we then sought original replicator permission, 
#since it would involve sharing their repo and write-up with a new student
#which wouldn't have been anticipated at original classtime

#we heard back from 29 students of the 49 we attempted to contact 
#(1 we couldn't contact due to email bouncing) 
#2 responses were negative, rest were positive, for 27 options

approved <- c("krauss2003","yeshurun2003", "daffner2000", "ngo2019",
              "child2018", "schechtman2010", "payne2008", "paxton2012",
              "hart2018", "lewis2015", "tarampi2016", "jara-ettinger2022",
              "porter_2016_1", "hopkins2016", "birch2007_2", "gong2019",
              "correll2007", "pilditch2019", "daw2011", "dehaene2009",
              "sofer2015", "todd2016_1", "chou2016", "mani2013", 
              "haimovitz2016_2", "haimovitz2016_1", "craig2014")
options <- filtered |> filter(target_lastauthor_year %in% approved)

#test_that("number of rows in options", {expect_equal(nrow(options),27)})

# students were then given this list and the opportunity to choose what to do

# projects that we believe are being rescued (as of Oct 23)

chosen <- c("birch2007_2", "jara-ettinger2022", "porter_2016_1",
            "hopkins2016", "yeshurun2003", "craig2014", "tarampi2016", 
            "mani2013", "krauss2003", "schechtman2010", "child2018",
            "haimovitz2016_1", "todd2016_1", "haimovitz2016_2", "payne2008", 
            "paxton2012", "dehaene2009", "chou2016", "ngo2019", "gong2019")

rescue_started <- options |> filter(target_lastauthor_year %in% chosen)

#test_that("number of rows in final sample", {expect_equal(nrow(final_sample),21)})

```

```{r}
d <- read_csv(here("data", "combined_data.csv")) |> 
  select(target_lastauthor_year, type, on_turk, repeated_measure, N, raw_stat, 
         same_direction, replication_score, closeness, subfield, target_year, stanford_internal, 
         open_data, open_materials, within_between, single_vignette, cost, name_pretty)

source(here("code","helper","parse_stats.R"))

parsed_d <- d |> 
  mutate(raw_stat=gsub(" ","",raw_stat),
         calc=pmap(list(raw_stat, within_between,N), do_parsing)) |> 
  unnest(cols=c(calc), names_sep="_") |> 
    mutate(
    calc_d_calc=case_when(
      type=="original" ~ abs(calc_d_calc),
      same_direction=="yes" ~ abs(calc_d_calc),
      same_direction=="no" ~ -abs(calc_d_calc),
      T ~ as.numeric(NA)
      ),
    calc_ES=case_when(
      type=="original" ~ abs(calc_ES),
      same_direction=="yes" ~ abs(calc_ES),
      same_direction=="no" ~ -abs(calc_ES),
      T ~ as.numeric(NA)
      ),
    type=factor(type, levels=c("original", "rep1", "rescue", "additional"))
    ) |> 
  rowwise()

num_proj <- parsed_d |> filter(type=="rescue") |> nrow()

cost <- parsed_d |> filter(type=="rescue") |> ungroup() |>  summarize(total=sum(cost, na.rm=T))
```

[Probably needs a better transition in] Here we report the results of `r num_proj` re-replication "rescue" projects which each re-replicated a study which had a failed or only partially-successful replication in @boyce2023. This is a non-random sample, and given the small number of projects, we provide descriptive statistics and discuss qualitatively some potential sources of differential re-replication outcomes. 



# Methods
PSYCH 251 is a graduate-level experimental methods class taught by MCF. In previous years, students have conducted replication projects, as reported in @boyce2023. In Fall 2023, students in PSYCH 251 were offered the option to do a "rescue" project where they re-replicated one of the unsuccessful replications from a previous year (students could also opt to do a normal replication instead). We report on the result of `r num_proj` rescue projects that opted to be part of the paper and completed data collection. 

A spreadsheet of projects, individual project write-ups (both replications and rescues), links to individual project data and analyses for rescue projects, and the analytic code for this paper are all available at OSF LINK GOES HERE. 

## Sample
The experiments that were re-replicated were a non-random sample of studies from @boyce2023. 
We created an initial list of `r nrow(filtered)` rescue-eligible studies that had received a subjective replication success score of 0, .25 or .5 (on a 0-1 scale) in @boyce2023, where the replication had a github repository available (github repositories were used starting in academic year 2015-2016), and where the original experiment had 200 or fewer participants (for feasibility reasons if we needed to increase power). We then contacted the replication project authors for permission to share their report and github repository with a new student and include it as a supplement on a resulting paper. This left `r length(approved)` options that were offered to the students. `r length(chosen)` students chose to do rescue projects; `r length(chosen)-num_proj` students took an incomplete or did not indicate interest in being part of the rescue paper, leaving a final sample of `r num_proj` rescue projects. 

## Procedure 
Students conducted their rescue projects over the course of the 10-week class. Once they had chosen a project we gave them access to the original replicators' write-up and repository, which often included the data, experiment code, and analytic code. In many cases, students were also given the contact information of the original replicator (a few original replicators opted not to be contacted by students). 

Students were required to think of reasons the original replication might not have worked, and address them if they could. A list of possible reasons and solutions LINK was given to students. Once students experimental designs and analytic plans were approved by TAs (VB and BP), students pre-registered and ran their samples. 

With one exception, samples were collected on Prolific (the rescue of @yeshurun2003 ran in-person on the Stanford student subject pool). We tried to power studies adequately (with a target of 2.5x original following @simonsohn2015), but due to cost constraints, not all studies were powered at this level. (See table TODO for post-exclusion sample sizes of original, replication, and rescues). Across the `r num_proj-1` studies, we spent $`r pluck(cost,1,1) |> round()`, for an average of $`r (pluck(cost,1,1)/(num_proj-1)) |> round()` per project. 



## Pre-registration
Our analysis plan was pre-registered after students had selected projects, but before final data collection on the projects. Each project was also individually pre-registered by the student conducting it. The overall analysis is at LINK, individual pre-registrations are linked from LINK. 

## Coding of results
We followed @boyce2023 in what study properties we measured and what measures of replication success we used. 

Each project was rated on the basis of subjective replication success both by MCF and by one of VB and BP. Disagreements were resolved through discussion. As a compliment to the overall subjective rating of success, we followed @boyce2023 in also doing a statistical comparison on one key measure of interest for each study. 

 We also recorded the same set of potential correlates that were used in @boyce2023 for original, replication, and rescue (these were already rated for original and replication). These potential correlates included features of the original study including the subfield of the study (cognitive v social v other psychology), it's publication year,  experimental design features including whether it was a within- or between- subject design, whether each condition was instantiated with one vignette or multiple, and how many items each participant saw, and whether there were open materials and open data. 
 
For the original study and each replication, we recorded the number of participants post-exclusions. For studies where some extra conditions were dropped, we count only the participants in the key conditions all replications had for comparability. For instance, if an original study compared between two critical conditions but also had a baseline control, we would not count the participants in the baseline condition if a replication did not include this condition. We also recorded whether each study was conducted online with a crowdsourced platform or not. 


# Results

Our primary question of interest is how many of these `r num_proj` rescue projects succeeded at replicating the results in the original study. When a replication fails to obtain the same results, one may have intuitions about what may have gone wrong -- these rescue projects test whether addressing these potential issues in fact works. 

```{r}
subj <- parsed_d |> filter(type=="rescue") |> mutate(binary_success=ifelse(replication_score>.5,1,0))

tallied <- subj|> group_by(replication_score) |> tally()

success <- filter(subj, binary_success==1)

d <- read_csv(here("data", "combined_data.csv")) |> filter(type=="rescue") 

irr <- cor.test(d$TA_rep_score, d$MCF_rep_score, method="spearman")$estimate
```

## Overall replication rate 
<!--* We will report the distribution of subjective replication success in our rescue sample

* If there is a mixture of projects that succeed and fail to replicate the original results, we will qualitatively describe differences that may have played a role. -->
All rescue projects were rated holistically for how well they replicated the original results. We thought about this in terms of how confident one would be to build on this line of work given the replication results, rather than focusing on any singular numeric result or significance cut-off.  All projects were rated both by the instructor (MCF) and by one of the TAs (VB or BP); the interrater reliability was `r irr |> round(3)`.  
Across the `r nrow(subj)` replications,  `r nrow(success)` mostly or fully replicated the original results according to the subjective replication ratings. `r pluck(tallied,2,1)` had a rating of 0, `r pluck(tallied, 2,2)` got a rating of  .75, and `r pluck(tallied, 2,3)` got a rating of 1. Thus, a first pass answer to the question "how often can a failed replication be salvaged?" is `r round(nrow(success)/nrow(subj)*100)`% of the time. TODO DO WE WANT TO BOOTSTRAP A CI ON THIS?

Given the mix of successful and unsuccessful rescues, we discuss a few projects where we have speculations about why they turned out the way they did. 

One of the rescues that went from a replication with score of 0 to a rescue with score of 1 was the rescue of @krauss2003. This study looked at the influence of a guided thinking on whether or not people gave correct justifications (drawn or written) for their answer on the Monty Hall problem. The original paper reported correct justification from 2/67 (`r round(2/67*100)`%) in the control condition and 13/34 (`r round(13/34*100)`%) in the guided thinking condition.  The  first replication struggled to recruit participants who were naive to the problem (an exclusion criterion), and many participants give very short text responses in the provided text box (only textual responses were allowed). The replication found 0/8 correct justifications in the control and 0/11 in the guided thinking condition. While we can't know for sure what caused the non-replication, there were clear problems observable from the small final sample and low-quality responses. The rescue targeted these issues by adding a pre-screen for naivete to the Monty Hall problem, switched the name of the problem (to reduce googling for answers), and had participants upload drawings for their justifications. Collectively, these changes brought the rescue closer to the intent of the original.  The rescue had  1/40 (`r round(1/40*100)`%) correct justifications in the control group and 6/35 (`r round(6/35*100)`%) in the guided thinking group. The rescue effect is smaller, but the overall pattern of results replicated, and the online adaptation in the rescue feels like it could be built on. 

Another successful replication was that of @ngo2019. Here, the original study found a large effect, and so the first replication, powering for 80% power on the reported effect, recruited a small sample of 12 people, and then failed to find the effect. The rescue, powered using 2.5x the original sample (as recommended by @simonsohn2015a), recovered a clear effect (albeit a much smaller one). There are reasons to think that some effect sizes in the literature may be inflated CITATIONS, and separately potential reasons that slight changes to experiments, or switches to online, could result in noisier samples (and thus smaller effect sizes). Thus, replications with smaller samples than the original (even if powered to the original effect size), may not be that diagnostic, and could potentially benefit from a re-replication. 

Not all rescues of small replications succeeded, however. @payne2008 was a study of the effects of sleep versus wake on memory consolidation that showed participants a number of images and then hours later (after either sleep or no sleep) measured their recall for parts of the images. The first replication struggled to recruit participants and only got 23 (the original had 48). The rescue attempted to recruit a larger sample (target 88), but due to difficulties getting participants to complete the second part of the experiment 12 hours after the first, the rescue only managed to recruit 23 people. The lesson here may be that sleep research is difficult to conduct online. 

(TODO other successes or failures we want to discuss? )

(could discuss tarampi as an example where there were potential issues, we fixed them and it still didn't work?)

## Correlates of rescue success

<!-- TODO some table of correlations between measures of rescue success rate!! The predictors used in https://osf.io/preprints/psyarxiv/dpyn6/ for exploratory analyses (the ones that need to be calculated) -->
```{r}

original <- parsed_d |> filter(type=="original") |> 
  rename_with(~str_c("original_",.), .cols=-target_lastauthor_year)

rep1 <- parsed_d |> filter(type=="rep1") |> select(target_lastauthor_year, rep_N=N)

for_cor <- subj |> left_join(original) |> left_join(rep1) |> 
  mutate(
  social=ifelse(subfield=="social", 1,0),
  other_psych=ifelse(subfield=="other-psych",1,0), 
  is_within=ifelse(within_between=="within", 1,0), 
  change_platform=ifelse(on_turk==original_on_turk, 0,1),
  log_trials=log(repeated_measure),
  log_sample=log(N),
  log_ratio_ss=log(N/original_N),
  rep_1_log_sample=log(rep_N),
  log_ratio_rep1_orig=log(rep_N/original_N),
  log_ratio_rescue_rep1=log(N/rep_N),
  open_data=ifelse(open_data=="yes",1,0),
  open_mat=ifelse(open_materials=="yes", 1,0),
  stanford=ifelse(stanford_internal=="yes",1,0)) |> 
  filter(!is.na(replication_score))


sub_cor <- function(var, stat = "estimate") {
  if (stat == "estimate") {
    cor.test(for_cor$replication_score, pull(for_cor, {{var}}))$estimate
  } else {
    cor.test(for_cor$replication_score, pull(for_cor, {{var}}))$p.value
  }
}


preds <- c("open_data",  "open_mat", "stanford", "change_platform", 
           "log_ratio_ss", "is_within", "single_vignette", "log_sample", 
           "log_trials", "social", "other_psych", "rep_1_log_sample", "log_ratio_rep1_orig", "log_ratio_rescue_rep1")

cors <- tibble(preds = preds) |>
  mutate(r = sapply(preds, function(x) sub_cor(x, stat = "estimate")),
         p = sapply(preds, function(x) sub_cor(x, stat = "p"))) |> 

  mutate(Predictors=factor(preds, levels=c("social", "other_psych", "is_within", "single_vignette", "change_platform", "open_data", "open_mat", "stanford", "pub_year", "log_trials", "log_sample", "log_ratio_ss", "rep_1_log_sample", "log_ratio_rep1_orig", "log_ratio_rescue_rep1"), labels=c("Social", "Other psych", "Within subjects", "Single vignette", "Switch to online", "Open data", "Open materials", "Stanford", "Publication year", "Log trials", "Log original sample size", "Log rescue/original sample size", "Log replication sample size", "Log replication/original sample size", "Log rescue/replication sample size"))) |> arrange(Predictors) |> select(Predictors, r, p)

library(kableExtra)
knitr::kable(cors, digits = 3, align='rcc', format="latex", booktabs=T, linesep="", caption="Correlations between an individual predictor and the subjective replication score of the rescue project.")


```

We ran correlations between the set of predictor variables used in @boyce2023 and the subjective replication scores of the rescues (TABLE WHATEVER). As the number of rescues is small, and many of these predictors are correlated, these correlations should not be overinterpreted.

Given that a couple of the successful rescues seemed to succeed in part because of larger sample sizes, we also added predictors related to the first replication sample size and the relative sizes of the samples (post-hoc, not pre-registered). 
While not significant, the strongest correlates of rescue success were open materials, a small sample size on the first replication, a small sample size on the first replication relative to the original sample size, and a large rescue sample size relative to the first replication.

Small replication samples relative to original and rescue could be due both to a) aiming for a small replication sample due to aiming for power for a reported large effect size or b) difficulties with recruitment or high exclusion rates leading to a smaller than intended sample. 

Two measures associated with how satisfying a replication attempt was are its sample size (relative to the original) and how close the replication was to the original. We show these measures (using the classification scheme of @lebel for closeness) in TABLE TODO. 

```{r}
parsed_d |> select(name_pretty, type, N, closeness, replication_score) |> 
  filter(type!="additional") |> 
  pivot_wider(names_from="type", values_from=c(N, closeness, replication_score)) |> 
  select(paper=name_pretty, replication_score_rescue, N_original, N_rep1, N_rescue,
         "Replication \\\\  closeness"=closeness_rep1, closeness_rescue) |> arrange(replication_score_rescue|> desc()) |> knitr::kable(align="lllll", format="latex", booktabs=T, linesep="", caption="Comparison of sample size for original, replication, and rescue samples and measures of closeness for replication and rescue samples.", col.names=c("", "", "Original", "Replication", "Rescue", "Replication", "Rescue")) |> add_header_above(c("Paper"=1, "Score"=1, "N"=3, "closeness"=2)) 

```

Aside from the suggestion that fixing sample size issues may have helped, it's unclear why some projects replicated this time and most did not.  


```{r, fig.cap="Standardized effect sizes of original studies, first replications, rescues, and additional replications if available. Due to the large effect size of a couple studies, large effect studies are shown in a separate panel. "}


#test on just one
colors <- c("original"="black", "rep1"="#377EB8", "rescue"="#E41A1C", "additional"="#984EA3")
shape <- c("original"=21,"rep1"=22,"rescue"=23,"additional"=24)

big <- parsed_d |> filter(type=="original") |> filter(calc_d_calc>1) |> select(target_lastauthor_year)
for_plotting_d <- parsed_d |> filter(!is.na(calc_d_calc)) |> 
  mutate(point=calc_d_calc, low=calc_d_calc-1.96*calc_d_calc_se, high=calc_d_calc+1.96*calc_d_calc_se) 



smd_scale_small <- ggplot(for_plotting_d |> anti_join(big),aes(x=reorder(name_pretty, point, FUN=max),y=point,ymin=low,ymax=high,shape=type, color=type, fill=type, group=desc(type)))+
  geom_errorbar(size=.5,width=0, position=position_dodge(width=.5))+
  geom_point(position=position_dodge(width=.5))+
 coord_flip()+
  scale_size_area()+
  geom_hline(yintercept=0,color="black")+
    scale_color_manual(values=colors, aesthetics=c("color", "fill"))+
  scale_shape_manual(values=shape)+
  theme(legend.position = "bottom", 
        panel.grid.major.x = element_blank(), 
        panel.grid.minor.x=element_blank()
        )+
  labs(y="Standarized mean difference", x="")

smd_scale_big <- ggplot(for_plotting_d |> inner_join(big),aes(x=reorder(name_pretty, point, FUN=max),y=point,ymin=low,ymax=high,shape=type, color=type, fill=type, group=desc(type)))+
  geom_errorbar(size=.5,width=0, position=position_dodge(width=.5))+
  geom_point(position=position_dodge(width=.5))+
 coord_flip()+
  scale_size_area()+
  geom_hline(yintercept=0,color="black")+
    scale_color_manual(values=colors, aesthetics=c("color", "fill"))+
    scale_shape_manual(values=shape)+
  theme(legend.position = "none", 
        panel.grid.major.x = element_blank(), 
        panel.grid.minor.x=element_blank()
        )+
  labs(y="Standarized mean difference", x="")

#smd_scale_big
#smd_scale_small

plot_grid(smd_scale_big, smd_scale_small, rel_heights=c(.3,1), nrow=2)
```

```{r, include=F, fig.cap="Standardized effect sizes of original studies, first replications, rescues, and additional replications if available. Due to the large effect size of a couple studies, large effect studies are shown in a separate panel. "}


#test on just one
colors <- c("original"="black", "rep1"="#377EB8", "rescue"="#E41A1C", "additional"="#984EA3")
shape <- c("original"=21,"rep1"=22,"rescue"=23,"additional"=24)





smd_scale <- ggplot(for_plotting_d,aes(x=reorder(name_pretty, point, FUN=max),y=point,ymin=low,ymax=high,shape=type, color=type, fill=type, group=desc(type)))+
  geom_errorbar(size=.5,width=0, position=position_dodge(width=.5))+
  geom_point(position=position_dodge(width=.5))+
 coord_flip()+
  scale_size_area()+
  geom_hline(yintercept=0,color="black")+
    scale_color_manual(values=colors, aesthetics=c("color", "fill"))+
  scale_shape_manual(values=shape)+
  theme(legend.position = "bottom", 
        panel.grid.major.x = element_blank(), 
        panel.grid.minor.x=element_blank()
        )+
  labs(y="Standarized mean difference", x="")


smd_scale

```

## Effect sizes 

In addition to looking at replication success overall, we also statistically compared the rescue, original replication, and original study on a key effect (we follow @boyce2023 in the determination of a key effect). When we were aware of additional direct replications (either from other class projects, or external replications in the literature), we also include these. 

We first compare the original and replications effect sizes on the key effect of interest. We report this comparison in standardized mean difference (SMD) units. One potential issue with comparisons using SMD is that noisier measures will have smaller standardized effect sizes even if the effect on the original scale is the same. The effect sizes are shown in FIGURE TODO. In general, the replication and rescue effect sizes were smaller than the original effect sizes, and in a couple cases the effects were in the opposite direction. 

```{r}
do_rma <- function(df) {
  if(!any(is.na(df$es))){
  r<- rma(yi=df$es, sei=df$es_se, slab=df$type)
  return(tibble(rma_est=r$beta[,1],rma_se=r$se))
  }
  return(tibble(rma_est=NA, rma_se=NA))
}

do_d_rma <- function(df) {
  if(!any(is.na(df$d))){
  r<- rma(yi=df$d, sei=df$d_se, slab=df$type)
  return(tibble(rma_d_est=r$beta[,1], rma_d_se=r$se))
  }
  return(tibble(rma_d_est=NA, rma_d_se=NA))

}


do_p_orig <- function(es, es_se, rep_es, rep_se){
  if(!is.na(es)&!is.na(es_se)&!is.na(rep_es)&!is.na(rep_se)){
    return(Replicate::p_orig(es, es_se**2,rep_es, t2=0.21**2, rep_se**2))
  }
  return(NA)
}

orig <- parsed_d |> filter(type=="original") |> select(target_lastauthor_year, d=calc_d_calc, d_se=calc_d_calc_se, es=calc_ES, es_se=calc_SE)

orig_v_others <- parsed_d |> select(target_lastauthor_year, type, d=calc_d_calc, d_se=calc_d_calc_se, es=calc_ES, es_se=calc_SE) |> 
  filter(type!="original") |> 
  group_by(target_lastauthor_year) |> 
  nest() |> 
  mutate(rma_result=map(data, do_rma),
         d_rma_result=map(data, do_d_rma)) |> 
  unnest_wider(rma_result) |> 
  unnest_wider(d_rma_result) |> 
  left_join(orig) |> 
  rowwise() |> 
  mutate(d_p_orig=do_p_orig(d, d_se, rma_d_est, rma_d_se))

orig_v_rescue <- parsed_d |>   filter(type=="rescue") |> 
  select(target_lastauthor_year, type, res_d=calc_d_calc, res_d_se=calc_d_calc_se, res_es=calc_ES, res_es_se=calc_SE) |> 

  left_join(orig) |> 
  rowwise() |> 
  mutate(d_p_orig=do_p_orig(d, d_se, res_d, res_d_se))


orig_v_not_rescue <- parsed_d |> select(target_lastauthor_year, type, d=calc_d_calc, d_se=calc_d_calc_se, es=calc_ES, es_se=calc_SE) |> 
  filter(type!="original", type!="rescue") |> 
  filter(target_lastauthor_year!="krauss2003") |> # have to avoid the divide by 0 error
  group_by(target_lastauthor_year) |> 
  nest() |> 
  mutate(rma_result=map(data, do_rma),
         d_rma_result=map(data, do_d_rma)) |> 
  unnest_wider(rma_result) |> 
  unnest_wider(d_rma_result) |> 
  left_join(orig) |> 
  rowwise() |> 
  mutate(d_p_orig=do_p_orig(d, d_se, rma_d_est, rma_d_se))

rescue_only <- parsed_d |> filter(type=="rescue") |> select(target_lastauthor_year, d=calc_d_calc, d_se=calc_d_calc_se, es=calc_ES, es_se=calc_SE)

rescue_v_reps <- parsed_d |> select(target_lastauthor_year, type, d=calc_d_calc, d_se=calc_d_calc_se, es=calc_ES, es_se=calc_SE) |> 
  filter(type!="original", type!="rescue") |> 
  filter(target_lastauthor_year!="krauss2003") |> # have to avoid the divide by 0 error
  group_by(target_lastauthor_year) |> 
  nest() |> 
  mutate(rma_result=map(data, do_rma),
         d_rma_result=map(data, do_d_rma)) |> 
  unnest_wider(rma_result) |> 
  unnest_wider(d_rma_result) |> 
  left_join(rescue_only) |> 
  rowwise() |> 
  mutate(d_p_orig=do_p_orig(d, d_se, rma_d_est, rma_d_se))

 all <- orig_v_others |> select(target_lastauthor_year, orig_v_other=d_p_orig) |> 
  left_join(orig_v_rescue |> select(target_lastauthor_year, orig_v_rescue=d_p_orig)) |> 
  left_join(orig_v_not_rescue |> select(target_lastauthor_year, orig_v_not_rescue=d_p_orig)) |> 
  left_join(rescue_v_reps |> select(target_lastauthor_year, rescue_v_reps=d_p_orig)) 

main_summ <- all |> ungroup() |> select(orig_v_other) |>
  summarize(median=median(orig_v_other, na.rm=T),
            low=quantile(orig_v_other, .25, na.rm=T),
            high=quantile(orig_v_other, .75, na.rm=T),
            less_than=sum(orig_v_other<.05, na.rm=T)/sum(!is.na(orig_v_other)))

rescue_summ <- all |> ungroup() |> select(orig_v_rescue) |>
  summarize(median=median(orig_v_rescue, na.rm=T),
            low=quantile(orig_v_rescue, .25, na.rm=T),
            high=quantile(orig_v_rescue, .75, na.rm=T),
            less_than=sum(orig_v_rescue<.05, na.rm=T)/sum(!is.na(orig_v_rescue)))

rep_summ <- all |> ungroup() |> select(orig_v_not_rescue) |>
  summarize(median=median(orig_v_not_rescue, na.rm=T),
            low=quantile(orig_v_not_rescue, .25, na.rm=T),
            high=quantile(orig_v_not_rescue, .75, na.rm=T),
            less_than=sum(orig_v_not_rescue<.05, na.rm=T)/sum(!is.na(orig_v_not_rescue)))

rescue_rep_summ <- all |> ungroup() |> select(rescue_v_reps) |>
  summarize(median=median(rescue_v_reps, na.rm=T),
            low=quantile(rescue_v_reps, .25, na.rm=T),
            high=quantile(rescue_v_reps, .75, na.rm=T),
            less_than=sum(rescue_v_reps<.05, na.rm=T)/sum(!is.na(rescue_v_reps)))
```

```{r}
names <- parsed_d |> select(target_lastauthor_year, name_pretty) |> unique()

all |> ungroup() |>  left_join(names) |> select(Paper=name_pretty, `All reps`=orig_v_other, "Rescue" = orig_v_rescue, "Non-rescue"=orig_v_not_rescue, "Other reps"=rescue_v_reps) |> knitr::kable(format="latex", booktabs=T, linesep="", caption="P-original values between different sets of experiments. The primary analysis is between the original result and the meta-analytic aggregation of all replications. All p-originals assume an imputed heterogeneity value of tau=.21.", digits=3) |> 
  add_header_above(c(" "=1, "Original and"=3, "Rescue and"=1)) |> add_header_above(c(" "=1, "P-original comparing between"=4))

  
```

## Consistency of effects 
We statistically compared the original effects with the replication effect sizes to determine their level of statistical consistency, that is whether these effects were likely to be drawn from the same distribution, given a certain level of heterogeneity. 

We note that statistical consistency does not always align with our intuitions of replication or with whether an effect provides support for the hypothesis [@patil2016]. A replication may be statistically consistent with the original effect but not provide any evidence for the claimed result. 

We use p-original to evaluate how consistent the original effect size is with the totality of replications (first replication, rescue, and additional if found). Because of the small number of replications, we impute the heterogeneity value of $\tau=.21$ SMD, which is the average level of heterogeneity found by @olsson2020 in prior multi-site replications in psychology. P-original measures the p-value on the null hypothesis that the original effect and the replications come from the same distribution. 

P-original values are shown in TABLE TODO. The median value of p_original was `r main_summ$median |> round(2)` [IQR: `r main_summ$low |> round(2)` - `r main_summ$high |> round(2)`]. `r round(main_summ$less_than*100)`% of the p_original values were less than .05, indicating by conventional thresholds a rejection of the null hypothesis that the original and the replications came from the same distribution for the given imputed level of heterogeneity. 

As secondary measures, we also calculated the p-original values for a) the original and the rescue, b) the original and non-rescue replications, and c) between the rescue and other replications. 
For a) the rescue versus the original, median value of p_original was `r rescue_summ$median |> round(2)` [IQR: `r rescue_summ$low |> round(2)` - `r rescue_summ$high|> round(2)`], and  `r round(rescue_summ$less_than*100)`% of the p_original values were less than .05.
For b) the non-rescue replications versus the original, median value of p_original was `r rep_summ$median |> round(2)` [IQR: `r rep_summ$low |> round(2)` - `r rep_summ$high |> round(2)`], and  `r round(rep_summ$less_than*100)`% of the p_original values were less than .05. For c) the rescue versus the other replications, median value of p_original was `r rescue_rep_summ$median |> round(2)` [IQR: `r rescue_rep_summ$low |> round(2)` - `r rescue_rep_summ$high |> round(2)`], and  `r round(rescue_rep_summ$less_than*100)`% of the p_original values were less than .05.

NOTE THERE ARE A LOT OF MISSING DATA HERE!!!

Visualizations of the statistical consistency between the original and each replication are shown in TODO FIGURE. The range around the open circle is the 95% predictive interval, representing the interval where, assuming that both are drawn from the same distribution, the replication will fall 95% of the time, given the precision (standard deviation) of the replication [@patil2016]. We again impute a heterogeneity value of $\tau=.21$ SMD for the construction of the prediction intervals. 

<!--* We will use p-original to evaluate how consistent the original effect size is with the totality of replications. We expect there to be a small number of replications, so we will impute the heterogeneity value as in https://osf.io/preprints/psyarxiv/dpyn6/. --> 
Overall, a number of these rescues and replications are not statistically consistent with the original effects allowing for this level of heterogeneity. 

```{r}
do_d_predInt_het <- function(df){
  if(!is.na(df$d)&!is.na(df$d_se)&!is.na(df$rep_d)&!is.na(df$rep_d_se)){
    yio=df$d
    vio=df$d_se**2
    yir=df$rep_d
    vir=df$rep_d_se**2
    t2=.21**2
    pooled.SE = sqrt(vio + vir + t2)
    PILo.sens = yio - qnorm(0.975) * pooled.SE
    PIHi.sens = yio + qnorm(0.975) * pooled.SE
    PIinside.sens = (yir > PILo.sens) & (yir < PIHi.sens)
    return(tibble(d_low=PILo.sens, d_high=PIHi.sens, d_inside=PIinside.sens))
  }
  return(tibble(d_low=NA,d_high=NA, d_inside=NA))
}

```

```{r}
orig <- parsed_d |> filter(type=="original") |> select(target_lastauthor_year, d=calc_d_calc, d_se=calc_d_calc_se, es=calc_ES, es_se=calc_SE)

predInts_het <- parsed_d |> 
  filter(type!="original") |> 
  group_by(target_lastauthor_year, name_pretty) |> 
  select(target_lastauthor_year, type, rep_d=calc_d_calc, rep_d_se=calc_d_calc_se, rep_es=calc_ES, rep_es_se=calc_SE) |> 
  left_join(orig) |> 
  group_by(target_lastauthor_year,type) |> 
  nest() |> 
  mutate(d_pred_int=map(data,do_d_predInt_het)) |> 
  unnest_wider(d_pred_int) |> 
  unnest_wider(data)


```

```{r, fig.caption="Plot of the prediction intervals of the original studies compared to replication results. The open circles are SMD effect sizes from the original studies, error bars are the prediction intervals, and filled shapes denote replication effect sizes. Prediction intervals in green contain the replication effect, those in grey do not. The largest effect sizes are separated in a separate panel to allow better visualization. "}
colors <- c("original"="black", "rep1"="#377EB8", "rescue"="#E41A1C", "additional"="#984EA3")
shape <- c("original"=21,"rep1"=22,"rescue"=23,"additional"=24)
col_code <- c("TRUE"="darkgreen", "FALSE"="black")

big <- predInts_het |> filter(d>1) |> select(target_lastauthor_year)

small_predInt <-  ggplot(predInts_het |> filter(!is.na(d_inside)) |> anti_join(big), aes(x=reorder(name_pretty, d, FUN=max), y=rep_d, ymin=d_low, ymax=d_high, fill=type, color=d_inside, group=desc(type), shape=type))+
  geom_errorbar(size=.5, width=.25, position=position_dodge(width=.4), alpha=.5)+
  geom_point(aes(y=d), position=position_dodge(width=.4), shape=1, alpha=.5)+
     geom_point(size=2.5, position=position_dodge(width=.4), color="transparent")+
  coord_flip()+
  scale_size_area()+
  geom_hline(yintercept=0,color="black")+
  theme(legend.position="bottom", 
        panel.grid.major.x = element_blank(), 
        panel.grid.minor.x=element_blank()
        )+
  scale_fill_manual(values=colors)+
  scale_shape_manual(values=shape)+
  scale_color_manual(values=col_code)+
  labs(y="predInts on SMD scale", x="")

big_predInt <-  ggplot(predInts_het |> filter(!is.na(d_inside)) |> inner_join(big), aes(x=reorder(name_pretty, d, FUN=max), y=rep_d, ymin=d_low, ymax=d_high, color=d_inside, fill=type, group=desc(type), shape=type))+
  geom_errorbar(size=.5,width=.25, position=position_dodge(width=.4), alpha=.5)+
  geom_point(aes(y=d), position=position_dodge(width=.4), shape=1, alpha=.5)+
     geom_point(size=2.5, position=position_dodge(width=.4), color="transparent")+
  coord_flip()+
  scale_size_area()+
  geom_hline(yintercept=0,color="black")+
  theme(legend.position="none", 
        panel.grid.major.x = element_blank(), 
        panel.grid.minor.x=element_blank()
        )+
  scale_fill_manual(values=colors)+
  scale_shape_manual(values=shape)+
  scale_color_manual(values=col_code)+
  labs(y="predInts on SMD scale", x="")

plot_grid(big_predInt, small_predInt, nrow=2, rel_heights = c(.3,1))

```

# Discussion
We presented the results of `r num_proj` new replications that attempted to "rescue" previous failed replications reported in @boyce2023 by identifying possible causes of non-replication and ameliorating them. `r nrow(success)` of these replications (`r round(nrow(success)/num_proj*100)`%) mostly or fully replicated. 

We don't have qualitative or quantitative explanations for why some replicated and some didn't. In a couple cases, increasing sample size and fixing internal validity issues in the replication seems to have led to a successful rescue (although we can't establish causality even in these cases). However, there were other studies that had small replication samples, or implementational deviations in the replication, and rescues were still unsuccessful. We can't predict what replication failures are likely to resolve given another try (or a more thoughtful try), beyond that suggestion that glaring problems and low samples may sometimes (but not always) be resolvable. 

Another pattern we observed was that the effect sizes of even the successful replications tended to be substantially smaller than the original effect.

[SAY something about statistical consistency] 

## Limitations
The reported rescue projects are a small sample of replications, and the effects explored are non-random, as they have been doubly selected by student interest. However, this non-random may be a useful selection bias as it is correlated with  how students choose what to work on. 

The authors of the rescue projects put substantial effort into trying to set up rescues that had a good chance of success, but projects were constrained by budget limitations, a short timeline, and primarily running online studies. These limitations are representative of the sort of resource limitations often faced by early-career researchers. That said, it is possible that different results might be obtained in better-resourced settings. Thus, we do not make statements about whether these studies are "true" or "false-positives", merely that they do not support cumulative research by early-career researchers under these conditions. 

We opened this paper with a question about what an early-career researcher should do given a failed replication. Should one try again or move on? From our sample of testing the "try again" approach, it seems that the odds of a re-replication working are low (consistent with @ebersole2020), so especially if there isn't a super clear failure mode to point to, another try probably won't fix it. 

# Acknowledgements {-}



# Author Contributions {-}

# References {-}

<!-- Use this magic to place references here. -->
<div id="refs"></div>

