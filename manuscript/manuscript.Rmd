---
title: "Estimating the likelihood of student replication success after an initial failure to replicate"
author:
  - name: Veronica Boyce
    affiliation: Stanford
    footnote:
      - corresp
  - name: TODO x a lot
    affiliation: Stanford
  - name: Michael C. Frank
    affiliation: Stanford
address:
  - code: Stanford
    address: Stanford University
footnote:
  - code: corresp
    text: "Corresponding author. Email: vboyce@stanford.edu"
bibliography: ["251rescue.bib"] # Replace with one or more of your own bibtex files. Better BibTeX for Zotero is your friend
csl: apa7.csl # Use any CSL style. See https://www.zotero.org/styles for a good list. Ignored if citation_package: natbib
link-citations: TRUE
output:
  bookdown::pdf_document2:
    toc: FALSE
    keep_tex: TRUE
    template: generic_article_template.tex
    #md_extensions: "-autolink_bare_uris"
    number_sections: TRUE
    citation_package: default # Can also be "natbib"
lang: en # Main document language in BCP47 format
geometry: "margin=25mm"
papersize: a4
#linestretch: 2 # for double spacing
endfloat: FALSE # Set to TRUE to turn on latex endfloat package to place figures and tables at end of document
# endfloatoption: # See endfloat documentation for more possibilities
numberlines: FALSE
authblk: TRUE # FALSE = author affiliations in footnotes; TRUE = author affiliations in a block below author names
footnotehyper: FALSE # TRUE will give you enhanced table footnote capabilities. Set to FALSE to be able to use French blocks. Needed due to what appears to be a latex bug.
urlcolor: blue
linkcolor: blue
citecolor: blue
graphics: TRUE # Needed to be able to include images
tables: TRUE # Needed to be able to include tables
# fancyhdr:
#   first:
#     #headleft: "REPORT-NO-XXXX"
#     headright: "Kaplan et al. (2021)"
#     headrulewidth: 0pt
#     #footleft: A left foot
#     footrulewidth: 0pt
#   subsequent:
#     #headleft: "NEXT-PAGE-HEADER-LEFT"
#     headright: "Kaplan et al. (2021)"
#     headrulewidth: 1pt
#     footrulewidth: 0pt
header-includes:
 - \usepackage{tikz}
 - \usetikzlibrary{positioning,chains}
 - \usepackage{setspace}\singlespacing
 - \renewcommand{\textfraction}{0.00}
 - \renewcommand{\topfraction}{1}
 - \renewcommand{\bottomfraction}{1}
 - \renewcommand{\floatpagefraction}{1}
 - \setcounter{topnumber}{3}
 - \setcounter{bottomnumber}{3}
 - \setcounter{totalnumber}{4}
editor_options: 
  markdown: 
    wrap: sentence
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE) # By default, hide code; set to TRUE to see code
#knitr::opts_chunk$set(fig.pos = 'th') # Places figures at top or here
knitr::opts_chunk$set(out.width = '100%', dpi=300,
                      fig.width=8, fig.width=8) # Figure resolution and size
knitr::opts_chunk$set(fig.env="figure") # Latex figure environment

options(knitr.table.format="latex") # For kable tables to work without setting format option

knitr::opts_chunk$set(echo=F, warning=F, message=F)#dev = "png", dev.args = list(type = "cairo-png")
 library("papaja")
library("bookdown")
library("rticles")
 library(here)
 library(tidyverse)
 library(brms)
library(tidybayes)
library(kableExtra)
library(viridis)
library(cowplot)
library(ggthemes)
library(reshape2)
library(knitr)
library(kableExtra)
library(metafor)
#r_refs("r-references.bib", append=F)
theme_set(theme_bw())
options(knitr.table.format = "pdf")

model_location="code/models"

```

```{r}


library(tidyverse)
library(testthat)
all_projects <- read_csv("https://raw.githubusercontent.com/vboyce/251-254-MA/main/data/raw_data.csv")

filtered <- all_projects |>
  mutate(sub_rep=ifelse(replicated_instructor_code==replicated_report_code, 
                 replicated_report_code, 
                 adjudicated_replication_code)) |> 
  filter(sub_rep %in% c(0,.25, .5)) |> 
  filter(include!="no") |> 
  filter(target_N <=200) |> 
  filter(academic_year %in% c("2015-2016", "2016-2017", "2017-2018", "2018-2019", 
                              "2019-2020", "2020-2021","2021-2022", "2022-2023")) 
# what we really want is whether there was a github, but that column is private and so not in the public raw data
# but 2015 is the first year where we have github links

#test_that("number of rows in filtered", {expect_equal(nrow(filtered),49)})

#given this sample, we then sought original replicator permission, 
#since it would involve sharing their repo and write-up with a new student
#which wouldn't have been anticipated at original classtime

#we heard back from 29 students of the 49 we attempted to contact 
#(1 we couldn't contact due to email bouncing) 
#2 responses were negative, rest were positive, for 27 options

approved <- c("krauss2003","yeshurun2003", "daffner2000", "ngo2019",
              "child2018", "schechtman2010", "payne2008", "paxton2012",
              "hart2018", "lewis2015", "tarampi2016", "jara-ettinger2022",
              "porter_2016_1", "hopkins2016", "birch2007_2", "gong2019",
              "correll2007", "pilditch2019", "daw2011", "dehaene2009",
              "sofer2015", "todd2016_1", "chou2016", "mani2013", 
              "haimovitz2016_2", "haimovitz2016_1", "craig2014")
options <- filtered |> filter(target_lastauthor_year %in% approved)

#test_that("number of rows in options", {expect_equal(nrow(options),27)})

# students were then given this list and the opportunity to choose what to do

# projects that we believe are being rescued (as of Oct 23)

chosen <- c("birch2007_2", "jara-ettinger2022", "porter_2016_1",
            "hopkins2016", "yeshurun2003", "craig2014", "tarampi2016", 
            "mani2013", "krauss2003", "schechtman2010", "child2018",
            "haimovitz2016_1", "todd2016_1", "haimovitz2016_2", "payne2008", 
            "paxton2012", "dehaene2009", "chou2016", "ngo2019", "gong2019")

rescue_started <- options |> filter(target_lastauthor_year %in% chosen)

#test_that("number of rows in final sample", {expect_equal(nrow(final_sample),21)})

```

```{r}
d <- read_csv(here("data", "combined_data.csv")) |> 
  select(target_lastauthor_year, type, on_turk, repeated_measure, N, raw_stat, 
         same_direction, replication_score, closeness, subfield, target_year, stanford_internal, 
         open_data, open_materials, within_between, single_vignette, cost, name_pretty)

source(here("code","helper","parse_stats.R"))

parsed_d <- d |> 
  mutate(raw_stat=gsub(" ","",raw_stat),
         calc=pmap(list(raw_stat, within_between,N), do_parsing)) |> 
  unnest(cols=c(calc), names_sep="_") |> 
    mutate(
    calc_d_calc=case_when(
      type=="original" ~ abs(calc_d_calc),
      same_direction=="yes" ~ abs(calc_d_calc),
      same_direction=="no" ~ -abs(calc_d_calc),
      T ~ as.numeric(NA)
      ),
    calc_ES=case_when(
      type=="original" ~ abs(calc_ES),
      same_direction=="yes" ~ abs(calc_ES),
      same_direction=="no" ~ -abs(calc_ES),
      T ~ as.numeric(NA)
      ),
    type=factor(type, levels=c("original", "rep1", "rescue", "additional"))
    ) |> 
  rowwise()

num_proj <- parsed_d |> filter(type=="rescue") |> nrow()

cost <- parsed_d |> filter(type=="rescue") |> ungroup() |>  summarize(total=sum(cost, na.rm=T))
```

<!---------------------- Abstract --------------------->

::: {.abstract data-latex="" lang="en"}
<!-- In order to do cumulative science, results need to be replicable. However, an individual direct replication could get a different result than an original ("replication failure") for a number of reasons, which vary in whether they would apply to a potential second replication. Given one failed replication, how likely is a second attempt at replication to succeed?-->

As a pragmatic matter, when a replication fails, scientists have to decide: do they make a second attempt at replication, or move on?
Given the relatively low rates of replication success reported in large scale replication projects, this choice point is one facing many scientists in psychology who attempt to replicate a study before building on it.
Here, we report on 17 re-replications of experiments with failed replicated reported in Boyce et al. (2023).
In 5/17 of these rescue projects (29%), the re-replication mostly or fully replicated the original results, albeit with smaller effect sizes.
While a few studies with failed replications were rescued with larger sample sizes and experimental tweaks, our overall results indicate that most failed replications stayed failed.
If the goal is to do cumulative science, one may wish to stop after one failed replication.
:::

```{=html}
<!-- Other Title ideas:

Can failed replications be rescued? Evidence from N re-replication experiments suggests rarely.

Try again? Attempts to rescue failed replications were generally unsuccessful

A low rate of successfully replicating experiments with a prior unsuccessful replication

Once is probably enough. Low replication rates when re-replicating N studies with failed replications. 

* may want to include "student" or "class"

Estimating the replicability of experiments with a prior unsuccessful replication

Estimating the likelihood of [student] replication success after an initial failure to replicate
-->
```
# Introduction

Imagine the scenario where you are a new graduate student, and you run a replication of a study in the literature that you are interested in building upon in your research.
The replication fails: perhaps the interaction you hoped for is directionally correct, but the point estimate is small and the confidence interval definitely overlaps 0, with a p-value of .3.
Or perhaps the interaction is numerically in the wrong direction, and also the main effects look different.
Alternatively the main effect of interest may be null, but you found a significant effect interaction that wasn't hypothesized.
Whatever the details of the replication failure, you are left with a question: Should you *try again* and run a re-replication, or should you *give up* and pick a different study to replicate and build on?

Running studies can be costly, so scientists must decide how to allocate their limited resources in the face of an uncertain literature.
Attempting to replicate existing studies is a common choice.
Scientists replicate studies for a variety of reasons, including calibrating their equipment, confirming a result before building on it, and engaging in meta-scientific practices [other reasons to highlight?
is there any citation for this?]
.
We focus on this middle motivation, where a scientist wants to build on a result, and thus wants it to succeed, but does not have strong reasons to believe or disbelieve the finding: it isn't a robust, well-tested effect, nor is it something they have particular suspicions about.
In an uncertain literature, we might not want to commit resources to building on an effect without first checking that we, with our resources and participant sample, can get the core effect we wish to build on.
Thus, scientists may want to re-replicate failed replications if the re-replication is likely to work and is indicative that future experiments in this tradition are likely to be informative.

This scientist-decision focused angle is different from the standard approach to replication which has take then the meta-scientific approach which has been primarily concerned with estimating the proportion of true effects in the literature.
Large-scale replication studies indicate that the literature is unreliable.

In psychology, large scale replication projects have tended to find replicability rates around half.
Across 100 studies with positive results, RP:P replicated 36 - 47% depending on the metric for replication success [@openscienceconsortium].
With large multi-site samples, Many Labs 1 replicated 11/13 effects (84.6%), Many Labs 2 replicated 14/28 effects (50%), and Many Labs 3 replicated 3/10 effects (30%) [@klein2014; @klein2018; @ebersole2016].
@camerer2018 included 21 behavioral social science studies and replicated 12-14 of them (57%-67%) depending on the metric used.
@boyce2023 reports an average replication score of 49% for 176 replications primary in psychology.
Psychology is not the only discipline where not everything replicates.
Large scale replication projects in other disciplines have found 39/97 (40%) for positive effects in cancer biology [@errington2021], 11/18 (61%) in economics [@camerer2016], and 31/40 (78%) for experimental philosophy [@cova2021].

TODO DISCUSSION OF THE LITERATURE What these replication rates say about the truth of the literature is beyond our scope, but these rates do indicate that scientists are far from guaranteed at being able to replicate effects from the literature in their own labs.
Thus, the question of what to do with a failed replication is a practical one that comes up frequently.

Replications can fail for many reasons.
Replication failure implies a certain time direction: first a "successful" study, usually one that got a certain statistically significant result, and (later) a "failed" replication that did not get this statistically significant result.
The replication failure framing implies a time direction (TODO THERE IS A CITATION FOR THIS IDEA RIGHT OF PRECEDENCE).
Taking out the time component, we can instead analyse a replication failure as two very-related studies with differing results.
Under this frame, the question becomes: what is different between these two studies that could account for the differing results?

[query: should I try to frame this as two that are symmetric , or should I stick to an original / rep framing?
I'm torn]

There are many potential reasons for differing results (see also @ebersole2020).
One option is that one of the studies had a statistically unlikely result (either through chance or p-hacking).
The result could be sensitive to some of the exact conditions and time when the two studies were run ("hidden moderators") and lack generalizability.
Reported effect sizes might be biased estimates due to some combination of heterogeneity, p-hacking, luck, underpowering, and publication bias.
One or both studies might be underpowered, potentially due to believing the effect was larger, or unexpected attrition in the sample.

Studies might differ in how they instantiated the question of interest, and this might cause different results.
For example if a manipulation check failed, that could be a proximate cause of difference, which also raises questions about why the manipulation check failed.
Differences in materials or instructions might turn out to be crucial to the effect.
A difference in experiment platforms (ex. online versus in-person) might mean that an implementation doesn't fit the platform, or the data quality controls and attention checks may not be appropriate to the sample population.
Some trivial changes intended to adapt the experimental question to the setting might actually change the result.
If there are differences in time, place, or subject population between the two studies, there may need to be differences in the materials, instructions, or procedure to provide a good *translation* -- too many changes could cause differing results, but so could not enough changes.

While one can theorize after the fact about potential causes of replication failure, in most circumstances it is hard to diagnose for certain what made for the difference in the results.
An exception might be if a specific error in one study or the other is found, such that the results were not what was thought (analytic error) or the data was not what was thought (experimental error).
Re-replication could serve as a way of triangulating on reasons for replication failure, as well as another chance for a scientist to get a working protocol that picks up the effect of interest.
However, re-replication has not been studied much, so it is uncertain how definitive a replication failure is.
That is, conditioned on a failed replication, how likely is a re-replication to succeed?
Re-replication was tested in a sample of 10 psychology studies in @ebersole2020.
After @openscienceconsortium published their results with a replication rate of TODO, @gilbert2016 raised concerns that methodological differences were the problem.
In 11 of the OSF studies, concerns about protocol fidelity had been raised by original authors prior to data-collection (and thus not contingent on results).
Of these 11 with concerns, 10 failed to obtain significant results in the RP:P replication [@openscienceconsortium].
In a follow-up, @ebersole2020 re-replicated 10 of these 11 in larger samples, using both the RP:P protocol and a new protocol revised under advice of the original authors or other experts, thus testing whether larger sample sizes and/or "better" methodologies would "rescue" the failed RP:P replications.
The result: 0 of the RP:P protocols found significant results, 2 of the 10 revised protocols did (but not the 1 that had a significant result in the original RP:P sample).
The replication effect size was much smaller CITE NUMBER [@ebersole2020].
We could frame @ebersole2020 as an attempt to rescue the original effects of the 9 studies with significant original effects and non-significant RP:P effects by a combination of high power and design tweaks.
This suggests a successful re-replication rate of 2/9 (22%) for studies with non-significant first replications, under fairly favorable, high resource circumstances.

Returning to the question of potential causes of non-replication, what do reasons for replication failure say about the success of another replication?
For our purposes, we're considering another replication with roughly the same environment, knowledge, and resources of the first replication, but with the benefit of hindsight on the first replication, as this is the situation scientists considering doing a re-replicaton for cumulative science are in.

Factors that suggest another failure include heterogeneity and lack-of-generalizability ("hidden moderator"), as the second replication is likely to be closer to the first replication than to the original in these random factors.
TODO CITE THAT PAPER about types of heterogeneity and which way they would fall An original result due to p-hacking or a lucky false positive is unlikely to re-replicate.
Sensitivity to environment or not working well online are unlikely to be fixable if the replications remain constrained to using online methods.
Similarly, if the result is specific to a special population, then a new replication is also likely to fail, unless it has gained access to this population.

Factors that may be fixable include sample size issues with the replication, assuming the re-replication can recruit a larger sample or reduce attrition.
It may be possible to fix underpoweredness and inflated effect size, at least to an extent.
If the effect size was inflated and the first replication powered for the effect, a better powered (but not huge) replication may be able to recover a smaller, but still convincing effect.
Failures due to differences between the materials may or may not be fixable.
If the original materials are still unavailable, a second replication is no better off.
If a second replication can get closer to the original, either by using closer materials, or better adapting the study to a new environment, the rescue might succeed at better approximating the results of the original.
A failed manipulation check failure may not be fixable, but it might at least be diagnoseable.

Overall, there are some potential causes of replication failure that may be resolvable with a similar level of resources, and also some potential causes of replication failure that are not resolvable, either at all, or given resource constraints.

After a failed replication, we might speculate on what caused the replication result, and what that would mean for future work on the topic.
As the @gilbert2016 and @ebersole2020 example showed, these speculations on causes and thus re-replication outcomes are not always correct.

While @ebersole2020 looked at re-replication success rate in a high resource setting with access to large, multi-site samples and expert input, most early career scientists do not have those kinds of resources to devote to re-replication attempts.
On the other hand, early career researchers may also be prone to a different distribution of causes of initial replication failure.

Here, we investigate re-replication in the context of graduate student replications.
Students in a graduate methods class on experimental methods were learning about best practices for experimental research.
One option for their class project was to "rescue" an experiment that a student in a prior year had previously failed to replicate (reported in @boyce2023).
Thus, students were likely to be looking at both the original paper and the failed replication through the lens of best practices to see what they could fix.
[TODO Mike mentioned something about a sampling side box on experimentology that VB is having trouble finding]

In the present paper, we report the results of `r num_proj` re-replication "rescue" projects which each re-replicated a study which had a failed or only partially-successful replication in @boyce2023.
This is a non-random sample, and given the small number of projects, we provide descriptive statistics and discuss qualitatively some potential sources of differential re-replication outcomes.
[possibly drop last sentence?]

# Methods

PSYCH 251 is a graduate-level experimental methods class taught by MCF.
In previous years, students have conducted replication projects, as reported in @boyce2023.
In Fall 2023, students in PSYCH 251 were offered the option to do a "rescue" project where they re-replicated one of the unsuccessful replications from a previous year.
Students could also opt to do a normal replication instead.
We report on the result of `r num_proj` rescue projects that opted to be part of the paper and completed data collection.

A spreadsheet of projects, individual project write-ups (both replications and rescues), links to individual project data and analyses for rescue projects, and the analytic code for this paper are all available at TODO OSF LINK GOES HERE.

## Sample

The experiments that were re-replicated were chosen from studies that failed to replicate in @boyce2023.
We created an initial list of `r nrow(filtered)` rescue-eligible studies that had received a subjective replication success score of 0, .25 or .5 (on a 0-1 scale) in @boyce2023, where the replication had a github repository available (github repositories were used starting in academic year 2015-2016), and where the original experiment had 200 or fewer participants (to ensure we could afford to increase the sample size).
We then contacted the replication project authors for permission to share their report and github repository with a new student and include it as a supplement on a resulting paper.
This left `r length(approved)` options for the students to choose from.
`r length(chosen)` students chose to do rescue projects.
`r length(chosen)-num_proj` students took an incomplete or did not indicate interest in being part of the rescue paper, leaving a final sample of `r num_proj` rescue projects.

```{r}
rel_size <- parsed_d |> filter(type %in% c("original", "rescue")) |> select(target_lastauthor_year, type, N)  |> 
  pivot_wider(names_from=type, values_from=N) |> mutate(multiplier=rescue/original)

lowest <- quantile(rel_size$multiplier, 0)
low <- quantile(rel_size$multiplier, .25)
mid <- quantile(rel_size$multiplier, .5)
m <- mean(rel_size$multiplier)
high <- quantile(rel_size$multiplier, .75)
highest <- quantile(rel_size$multiplier, 1)

```

## Procedure

Students conducted their rescue projects over the course of the 10-week class.
Once they had chosen a project we gave them access to the original replicators' write-up and repository, which often included the data, experiment code, and analytic code.
In many cases, students were also given the contact information of the original replicator (a few original replicators opted not to be contacted by students).

Students were required to think of reasons the original replication might not have worked, and address them if they could.
A list of possible reasons and solutions TODO LINK was given to students.
In general, we encouraged students to add manipulation checks as appropriate, and better adapt materials to online studies.
For instance, the rescue of @paxton2012 switched from using the CRT which is overused in online samples for the newer TODO.
The rescue of @jara-ettinger2022 discovered that the replication had accidentally used the drawn version of the stimuli rather than the photographic stimuli used in the original and reverted to the photographic stimuli.
@tarampi2016 original had participants indicate their answers (left or right) on a piece of paper in a timed navigation task, the replication had them indicate by clicking a drop down, and rescue went with press keyboard keys to indicate.
TODO others to note?
Once students experimental designs and analytic plans were approved by TAs (VB and BP), students pre-registered and ran their samples.

With one exception, samples were collected on Prolific (the rescue of @yeshurun2003 ran in-person on the Stanford student subject pool).
We tried to power studies adequately (with a target of 2.5x original following @simonsohn2015), but due to cost constraints, not all studies were powered at this level.
The rescues had on average `r round(m,2)` times the original sample post-exclusion (median: `r round(mid,2)`, IQR: `r round(low,2)` - `r round(high,2)`, minimum: `r round(lowest, 2)`, maximum: `r round(highest,2)`, see Table \@ref(tab:tab-size) for all sample sizes).
Across the `r num_proj-1` Prolific studies, we spent \$`r pluck(cost,1,1) |> round()`, for an average of \$`r (pluck(cost,1,1)/(num_proj-1)) |> round()` per project.

## Pre-registration

Our analysis plan was pre-registered after students had selected projects, but before final data collection on the projects.
Each project was also individually pre-registered by the student conducting it.
The overall analysis is at LINK, individual pre-registrations are linked from LINK.

We note one deviation from our pre-registration here: we pre-registered visual comparisons between original, first replication, and rescue projects.
Prediction intervals depend on both the original effect size and variance CITATION TODO, and also the variance of the comparison (replication) study.
Thus we cannot show *the* prediction interval for the original study, but would have to show a prediction interval between each pair of studies, which we thought would not offer clarity.

## Coding of results

We followed @boyce2023 in the properties of the studies we measure and how we quantify replication success.

Each project was rated on the basis of subjective replication success both by MCF and by one of VB and BP.
Disagreements were resolved through discussion.
As a compliment to the subjective rating of overall success, we stastically compared one key measure of interest for each study, following @boyce2023. In order to statistically compare the key measures, we needed effect sizes reported in the same way for each original study, replication, and re-replication.
When effects were not reported in consistent ways across original and replications, we recalculated effects from raw data when necessary to obtain comparable values.

We also recorded the same set of potential correlates that were used in @boyce2023 for original, replication, and rescue (these were already rated for original and replication).
These potential correlates included the subfield of the study (cognitive, social, or other psychology), its publication year, experimental design features including whether it was a within- or between- subject design, whether each condition was instantiated with one vignette or multiple, how many items each participant saw, and whether there were open materials and open data.

For the original study and each replication, we recorded the number of participants post-exclusions.
For studies where some extra conditions were dropped, we count only the participants in the key conditions all replications had for comparability.
For instance, if an original study compared between two critical conditions but also had a baseline control, we would not count the participants in the baseline condition if a replication did not include this condition.
We also recorded whether each study was conducted online with a crowdsourced platform or not.

# Results

Our primary question of interest is how many of these `r num_proj` rescue projects succeeded at replicating the results in the original study.
When a replication fails to obtain the same results, one may have intuitions about what may have gone wrong -- these rescue projects test how often addressing these potential issues in fact works.

```{r}
subj <- parsed_d |> filter(type=="rescue") |> mutate(binary_success=ifelse(replication_score>.5,1,0))

tallied <- subj|> group_by(replication_score) |> tally()

success <- filter(subj, binary_success==1)

d <- read_csv(here("data", "combined_data.csv")) |> filter(type=="rescue") 

irr <- cor.test(d$TA_rep_score, d$MCF_rep_score, method="spearman")$estimate

bootstrap <- subj$binary_success |> Hmisc::smean.cl.boot()
```

```{r}
boyce2023_sample <- read_csv(here("data/boyce_2023_data.csv")) |> select(sub_rep)

percent_rep <- boyce2023_sample |> mutate(is_rep=ifelse(sub_rep>.5,1,0)) |> summarize(m=mean(is_rep))

overall <- (percent_rep[1])+(1-percent_rep[1])*nrow(success)/nrow(subj)
```

## Overall replication rate

```{=html}
<!--* We will report the distribution of subjective replication success in our rescue sample

* If there is a mixture of projects that succeed and fail to replicate the original results, we will qualitatively describe differences that may have played a role. -->
```
All rescue projects were rated holistically for how well they replicated the original results.
We thought about replication in terms of how confident one would be to build on the line of work given the replication results, rather than focusing on any singular numeric result or significance cut-off.
All projects were rated both by the instructor (MCF) and by one of the TAs (VB or BP); the interrater reliability was `r irr |> round(3)`.

Across the `r nrow(subj)` replications, `r nrow(success)` mostly or fully replicated the original results according to the subjective replication ratings.
`r pluck(tallied,2,1)` had a rating of 0, `r pluck(tallied, 2,2)` got a rating of .75, and `r pluck(tallied, 2,3)` got a rating of 1.
Thus, a first pass answer to the question "how often can a failed replication be salvaged?" is `r round(nrow(success)/nrow(subj)*100)`% (bootstrapped 95% CI: `r round(bootstrap[2]*100)`% - `r round(bootstrap[3]*100)`%) of the time.

In the original replication sample from @boyce2023, `r boyce2023_sample |> filter(sub_rep>.5) |> nrow()` out of `r boyce2023_sample |> nrow()` replications (`r round(percent_rep*100)`%) mostly or fully replicated (i.e. received a subjective replication score of .75 or 1).
Note that @boyce2023 report the average replication score as a percent success (49%), but given that we considered studies with a subjective score of .5 as eligible to be rescued, we recomputed the success rate when scores of 0-.5 as failures and .75 and 1 as successes.
If the re-replication rate in our sample is representative of the re-replication rate for the initially non-replication studies, then the combined chance of mostly or fully replicating in a first replication or a follow-up replication is `r round(overall*100)`%.

Given the mix of successful and unsuccessful rescues, we discuss a few projects where we have speculations about why they turned out the way they did.

One of the rescues that went from a replication with score of 0 to a rescue with score of 1 was the rescue of @krauss2003.
This study looked at the influence of a guided thinking on whether or not people gave correct justifications (drawn or written) for their answer on the Monty Hall problem.
The original paper reported correct justification from 2/67 (`r round(2/67*100)`%) in the control condition and 13/34 (`r round(13/34*100)`%) in the guided thinking condition.
The first replication struggled to recruit participants who were naive to the problem (an exclusion criterion), and many participants give very short text responses in the provided text box (only textual responses were allowed).
The replication found 0/8 correct justifications in the control and 0/11 in the guided thinking condition.
While we can't know for sure what caused the non-replication, there were clear problems observable from the small final sample and low-quality responses.
The rescue targeted these issues by adding a pre-screen for naivete to the Monty Hall problem, switched the name of the problem (to reduce googling for answers), and had participants upload drawings for their justifications.
Collectively, these changes brought the rescue closer to the intent of the original.
The rescue had 1/40 (`r round(1/40*100)`%) correct justifications in the control group and 6/35 (`r round(6/35*100)`%) in the guided thinking group.
The rescue effect is smaller, but the overall pattern of results replicated, and the online adaptation in the rescue feels like it could be built on.

Another successful replication was that of @ngo2019.
Here, the original study found a large effect.
The first replication, powering for 80% power on the reported effect, recruited a small sample of 12 people.
It failed to find the effect.
The rescue, powered using 2.5x the original sample (as recommended by @simonsohn2015), recovered a clear effect (albeit a much smaller one).
There are reasons to think that some effect sizes in the literature may be inflated TODO CITATIONS, and separate reasons that slight changes to experiments or switches to online could result in noisier samples and thus smaller effect sizes.
Therefore, replications with smaller samples than the original (even if powered to the original effect size), may not be very diagnostic, and could potentially benefit from a re-replication.

Not all rescues of small replications succeeded, however.
@payne2008 was a study of the effects of sleep versus wake on memory consolidation that showed participants a number of images and then hours later (after either sleep or no sleep) measured their recall for parts of the images.
The first replication struggled to recruit participants and only got 23 (the original had 48).
The rescue attempted to recruit a larger sample (target 88), but due to difficulties getting participants to complete the second part of the experiment 12 hours after the first, the rescue only managed to recruit 23 people.
The lesson here may be that sleep research is difficult to conduct online.
However, an online replication by @denis2022 reports qualitatively similar (but quantitatively smaller) results to @payne2008 on related but not identical measures.

(TODO other successes or failures we want to discuss? ) (Ben: Emily's project was interesting: the rescue succeeded but with big stimulus-level random effects. Verity's project was also interesting: the previous replication, as well as another in the literature, showed similar but smaller findings and it really looked like she could get the effect with a bigger sample, but she ended up finding totally different results.)

(could discuss tarampi as an example where there were potential issues, we fixed them and it still didn't work?)

## Correlates of rescue success

```{r cor-table}

original <- parsed_d |> filter(type=="original") |> 
  rename_with(~str_c("original_",.), .cols=-target_lastauthor_year)

rep1 <- parsed_d |> filter(type=="rep1") |> select(target_lastauthor_year, rep_N=N)

for_cor <- subj |> left_join(original) |> left_join(rep1) |> 
  mutate(
  social=ifelse(subfield=="social", 1,0),
  other_psych=ifelse(subfield=="other-psych",1,0), 
  is_within=ifelse(within_between=="within", 1,0), 
  change_platform=ifelse(on_turk==original_on_turk, 0,1),
  log_trials=log(repeated_measure),
  log_sample=log(N),
  log_ratio_ss=log(N/original_N),
  rep_1_log_sample=log(rep_N),
  log_ratio_rep1_orig=log(rep_N/original_N),
  log_ratio_rescue_rep1=log(N/rep_N),
  open_data=ifelse(open_data=="yes",1,0),
  open_mat=ifelse(open_materials=="yes", 1,0),
  stanford=ifelse(stanford_internal=="yes",1,0)) |> 
  filter(!is.na(replication_score))


sub_cor <- function(var, stat = "estimate") {
  if (stat == "estimate") {
    cor.test(for_cor$replication_score, pull(for_cor, {{var}}))$estimate
  } else {
    cor.test(for_cor$replication_score, pull(for_cor, {{var}}))$p.value
  }
}


preds <- c("open_data",  "open_mat", "stanford", "change_platform", 
           "log_ratio_ss", "is_within", "single_vignette", "log_sample", 
           "log_trials", "social", "other_psych", "rep_1_log_sample", "log_ratio_rep1_orig", "log_ratio_rescue_rep1")

cors <- tibble(preds = preds) |>
  mutate(r = sapply(preds, function(x) sub_cor(x, stat = "estimate")),
         p = sapply(preds, function(x) sub_cor(x, stat = "p"))) |> 

  mutate(Predictors=factor(preds, levels=c("social", "other_psych", "is_within", "single_vignette", "change_platform", "open_data", "open_mat", "stanford", "pub_year", "log_trials", "log_sample", "log_ratio_ss", "rep_1_log_sample", "log_ratio_rep1_orig", "log_ratio_rescue_rep1"), labels=c("Social", "Other psych", "Within subjects", "Single vignette", "Switch to online", "Open data", "Open materials", "Stanford", "Publication year", "Log trials", "Log original sample size", "Log rescue/original sample size", "Log replication sample size", "Log replication/original sample size", "Log rescue/replication sample size"))) |> arrange(Predictors) |> select(Predictors, r, p)

library(kableExtra)
knitr::kable(cors, digits = 3, align='rcc', format="latex", booktabs=T, linesep="", caption="Correlations between an individual predictor and the subjective replication score of the rescue project.")


```

We ran correlations between the set of predictor variables used in @boyce2023 and the subjective replication scores of the rescues.
We also added some (non-preregistered) predictors related to the sample size of the first replication, after seeing the successful re-replications of @ngo2019 and @krauss2003.

All of the correlations are presented in Figure \@ref(tab:cor-table).
As the number of rescues is small, and many of these predictors are correlated, we caution against over-interpretation.
The strongest correlates of rescue success were open materials, a small sample size on the first replication, a small sample size on the first replication relative to the original sample size, and a large rescue sample size relative to the first replication.
None of these effects meet the conventional significance threshold.

Small replication samples relative to original and rescue could be due to both a) powering a replication according to a reported large effect size or b) difficulties with recruitment or high exclusion rates leading to a smaller than intended sample.
Since relative sizes of the studies may play a role in replication success and how probative replications are, we show the sample sizes in Table \@ref(tab:tab-size).

An additional factor that influences the interpretation of a replication is how close the replication's methods were to the original.
In the rescue projects, we aimed to have methods be as close as was feasible or appropriate.
(As discussed above, changes in time, population, or setting may necessitate adaptation of a study to maximize fidelity.) However, rescue projects varied in how close the re-replications actually were, often due to limitations in the availability of original stimuli and original instructions, in addition to the use of primarily online subject pools.
Table \@ref(tab:tab-size) shows the closeness of each re-replication according to the classification scheme from @lebel2018.

```{r tab-size}
parsed_d |> select(name_pretty, type, N, closeness, replication_score) |> 
  filter(type!="additional") |> 
  pivot_wider(names_from="type", values_from=c(N, closeness, replication_score)) |> 
  select(paper=name_pretty, replication_score_rescue, N_original, N_rep1, N_rescue,
         "Replication \\\\  closeness"=closeness_rep1, closeness_rescue) |> arrange(replication_score_rescue|> desc()) |> knitr::kable(align="lllll", format="latex", booktabs=T, linesep="", caption="Comparison of sample size for original, replication, and rescue samples and measures of closeness for replication and rescue samples.", col.names=c("", "", "Original", "Replication", "Rescue", "Replication", "Rescue")) |> add_header_above(c("Paper"=1, "Score"=1, "N"=3, "closeness"=2)) 

```

Overall, we do not have a clear picture of why certain studies replicated in the rescue sample and others did not, other than a few cases where fixing sample size issues may have helped.

```{r smd, fig.cap="Standardized effect sizes of original studies, first replications, rescues, and additional replications if available. Due to the large effect size of a couple studies, large effect studies are shown in a separate panel. "}


#test on just one
colors <- c("original"="black", "rep1"="#377EB8", "rescue"="#E41A1C", "additional"="#984EA3")
shape <- c("original"=21,"rep1"=22,"rescue"=23,"additional"=24)

big <- parsed_d |> filter(type=="original") |> filter(calc_d_calc>1) |> select(target_lastauthor_year)
for_plotting_d <- parsed_d |> filter(!is.na(calc_d_calc)) |> 
  mutate(point=calc_d_calc, low=calc_d_calc-1.96*calc_d_calc_se, high=calc_d_calc+1.96*calc_d_calc_se) 



smd_scale_small <- ggplot(for_plotting_d |> anti_join(big),aes(x=reorder(name_pretty, point, FUN=max),y=point,ymin=low,ymax=high,shape=type, color=type, fill=type, group=desc(type)))+
  geom_errorbar(size=.5,width=0, position=position_dodge(width=.5))+
  geom_point(position=position_dodge(width=.5))+
 coord_flip()+
  scale_size_area()+
  geom_hline(yintercept=0,color="black")+
    scale_color_manual(values=colors, aesthetics=c("color", "fill"))+
  scale_shape_manual(values=shape)+
  theme(legend.position = "bottom", 
        panel.grid.major.x = element_blank(), 
        panel.grid.minor.x=element_blank()
        )+
  labs(y="Standarized mean difference", x="")

smd_scale_big <- ggplot(for_plotting_d |> inner_join(big),aes(x=reorder(name_pretty, point, FUN=max),y=point,ymin=low,ymax=high,shape=type, color=type, fill=type, group=desc(type)))+
  geom_errorbar(size=.5,width=0, position=position_dodge(width=.5))+
  geom_point(position=position_dodge(width=.5))+
 coord_flip()+
  scale_size_area()+
  geom_hline(yintercept=0,color="black")+
    scale_color_manual(values=colors, aesthetics=c("color", "fill"))+
    scale_shape_manual(values=shape)+
  theme(legend.position = "none", 
        panel.grid.major.x = element_blank(), 
        panel.grid.minor.x=element_blank()
        )+
  labs(y="Standarized mean difference", x="")

#smd_scale_big
#smd_scale_small

plot_grid(smd_scale_big, smd_scale_small, rel_heights=c(.3,1), nrow=2)
```

```{r , include=F, fig.cap="Standardized effect sizes of original studies, first replications, rescues, and additional replications if available. Due to the large effect size of a couple studies, large effect studies are shown in a separate panel. "}


#test on just one
colors <- c("original"="black", "rep1"="#377EB8", "rescue"="#E41A1C", "additional"="#984EA3")
shape <- c("original"=21,"rep1"=22,"rescue"=23,"additional"=24)





smd_scale <- ggplot(for_plotting_d,aes(x=reorder(name_pretty, point, FUN=max),y=point,ymin=low,ymax=high,shape=type, color=type, fill=type, group=desc(type)))+
  geom_errorbar(size=.5,width=0, position=position_dodge(width=.5))+
  geom_point(position=position_dodge(width=.5))+
 coord_flip()+
  scale_size_area()+
  geom_hline(yintercept=0,color="black")+
    scale_color_manual(values=colors, aesthetics=c("color", "fill"))+
  scale_shape_manual(values=shape)+
  theme(legend.position = "bottom", 
        panel.grid.major.x = element_blank(), 
        panel.grid.minor.x=element_blank()
        )+
  labs(y="Standarized mean difference", x="")


smd_scale

```

## Effect sizes

As a complement to the holistic, qualitative replication ratings used above, we also statistically compared the effect sizes of the rescue, original replication, and original study on one key measure per study.
We followed @boyce2023 in determining a key measure for each study.
When we were aware of additional direct replications (either from other class projects, or external replications in the literature), we also consider the effect sizes obtained in these additional replications.

We standardized all effect sizes into standardized mean difference (SMD) units.
One potential issue with comparisons using SMD is that noisier measures will have smaller standardized effect sizes even if the effect on the original scale is the same.
In general, the replication and rescue effect sizes were smaller than the original effect sizes, and in a few cases the effects were in the opposite direction (Figure \@ref(fig:smd)).

```{r}

do_d_rma <- function(df) {
  if(!any(is.na(df$d))){
  r<- rma(yi=df$d, sei=df$d_se, slab=df$type)
  return(tibble(rma_d_est=r$beta[,1], rma_d_se=r$se))
  }
  return(tibble(rma_d_est=NA, rma_d_se=NA))

}


do_p_orig <- function(es, es_se, rep_es, rep_se){
  if(!is.na(es)&!is.na(es_se)&!is.na(rep_es)&!is.na(rep_se)){
    return(Replicate::p_orig(es, es_se**2,rep_es, t2=0.21**2, rep_se**2))
  }
  return(NA)
}

orig <- parsed_d |> filter(type=="original") |> select(target_lastauthor_year, d=calc_d_calc, d_se=calc_d_calc_se)

orig_v_others <- parsed_d |> select(target_lastauthor_year, type, d=calc_d_calc, d_se=calc_d_calc_se) |> 
  filter(type!="original") |> 
  group_by(target_lastauthor_year) |> 
  nest() |> 
  mutate(d_rma_result=map(data, do_d_rma)) |> 
  unnest_wider(d_rma_result) |> 
  left_join(orig) |> 
  rowwise() |> 
  mutate(d_p_orig=do_p_orig(d, d_se, rma_d_est, rma_d_se))

orig_v_rescue <- parsed_d |>   filter(type=="rescue") |> 
  select(target_lastauthor_year, type, res_d=calc_d_calc, res_d_se=calc_d_calc_se) |> 
  left_join(orig) |> 
  rowwise() |> 
  mutate(d_p_orig=do_p_orig(d, d_se, res_d, res_d_se))


orig_v_not_rescue <- parsed_d |> select(target_lastauthor_year, type, d=calc_d_calc, d_se=calc_d_calc_se) |> 
  filter(type!="original", type!="rescue") |> 
  group_by(target_lastauthor_year) |> 
  nest() |> 
  mutate(d_rma_result=map(data, do_d_rma)) |> 
  unnest_wider(d_rma_result) |> 
  left_join(orig) |> 
  rowwise() |> 
  mutate(d_p_orig=do_p_orig(d, d_se, rma_d_est, rma_d_se))

rescue_only <- parsed_d |> filter(type=="rescue") |> select(target_lastauthor_year, d=calc_d_calc, d_se=calc_d_calc_se)

rescue_v_reps <- parsed_d |> select(target_lastauthor_year, type, d=calc_d_calc, d_se=calc_d_calc_se) |> 
  filter(type!="original", type!="rescue") |> 
  group_by(target_lastauthor_year) |> 
  nest() |> 
  mutate(d_rma_result=map(data, do_d_rma)) |> 
  unnest_wider(d_rma_result) |> 
  left_join(rescue_only) |> 
  rowwise() |> 
  mutate(d_p_orig=do_p_orig(d, d_se, rma_d_est, rma_d_se))

 all <- orig_v_others |> select(target_lastauthor_year, orig_v_other=d_p_orig) |> 
  left_join(orig_v_rescue |> select(target_lastauthor_year, orig_v_rescue=d_p_orig)) |> 
  left_join(orig_v_not_rescue |> select(target_lastauthor_year, orig_v_not_rescue=d_p_orig)) |> 
  left_join(rescue_v_reps |> select(target_lastauthor_year, rescue_v_reps=d_p_orig)) 

main_summ <- all |> ungroup() |> select(orig_v_other) |>
  summarize(median=median(orig_v_other, na.rm=T),
            low=quantile(orig_v_other, .25, na.rm=T),
            high=quantile(orig_v_other, .75, na.rm=T),
            less_than=sum(orig_v_other<.05, na.rm=T)/sum(!is.na(orig_v_other)))

rescue_summ <- all |> ungroup() |> select(orig_v_rescue) |>
  summarize(median=median(orig_v_rescue, na.rm=T),
            low=quantile(orig_v_rescue, .25, na.rm=T),
            high=quantile(orig_v_rescue, .75, na.rm=T),
            less_than=sum(orig_v_rescue<.05, na.rm=T)/sum(!is.na(orig_v_rescue)))

rep_summ <- all |> ungroup() |> select(orig_v_not_rescue) |>
  summarize(median=median(orig_v_not_rescue, na.rm=T),
            low=quantile(orig_v_not_rescue, .25, na.rm=T),
            high=quantile(orig_v_not_rescue, .75, na.rm=T),
            less_than=sum(orig_v_not_rescue<.05, na.rm=T)/sum(!is.na(orig_v_not_rescue)))

rescue_rep_summ <- all |> ungroup() |> select(rescue_v_reps) |>
  summarize(median=median(rescue_v_reps, na.rm=T),
            low=quantile(rescue_v_reps, .25, na.rm=T),
            high=quantile(rescue_v_reps, .75, na.rm=T),
            less_than=sum(rescue_v_reps<.05, na.rm=T)/sum(!is.na(rescue_v_reps)))
```

```{r tab-porig}
names <- parsed_d |> select(target_lastauthor_year, name_pretty) |> unique()

all |> ungroup() |>  left_join(names) |> select(Paper=name_pretty, `All reps`=orig_v_other, "Rescue" = orig_v_rescue, "Non-rescue"=orig_v_not_rescue, "Other reps"=rescue_v_reps) |> knitr::kable(format="latex", booktabs=T, linesep="", caption="P-original values between different sets of experiments. The primary analysis is between the original result and the meta-analytic aggregation of all replications. All p-originals assume an imputed heterogeneity value of tau=.21.", digits=3) |> 
  add_header_above(c(" "=1, "Original and"=3, "Rescue and"=1)) |> add_header_above(c(" "=1, "P-original comparing between"=4))

  
```

Scientists' intuitions about whether a replication is successful and whether an effect provides support for a hypothesis (including heuristic cutoffs like p\<.05) do not always align with measures of statistical consistency [@patil2016].
For instance, two studies may both find that condition 1 results in a significantly higher outcome measure than condition 2, but the effect magnitudes may be sufficiently different that they are statistically unlikely to have come from the same population.
On the other hand, one study may find a statistically significant, but imprecisely estimated effect in a small sample, and second study may find a near-zero (null) effect, but the effect estimates from the two studies may be statistically compatible, despite one supporting a hypothesized difference and the other not.

[Ben: I think the paragraph below could use more explanation about what $p$-original is, but I'm not confident enough in my understanding of it to write it myself.]

Here, we measure statistical consistency using $p$-original, the $p$-value on the null hypothesis that two effects come from the same distribution.
As our primary comparison, we compare the effect size of the original study to the meta-analytic effect of the totality of the replications (first replication, rescue, and additional if found).
Because there is only a small number of replications per study, we impute the heterogeneity value of $\tau=.21$ SMD, which is the average level of heterogeneity found by @olsson-collentine2020 in prior multi-site replications in psychology.

[ query: should we bootstrap any of the following?
]

The median value of $p$-original between an original study and its replications was `r main_summ$median |> round(2)` [IQR: `r main_summ$low |> round(2)` - `r main_summ$high |> round(2)`].
`r round(main_summ$less_than*100)`% of the $p$-original values were less than .05, indicating by conventional thresholds a rejection of the null hypothesis that the original and the replications came from the same distribution for the given imputed level of heterogeneity.
Individual $p$-original values for each study are shown in Table \@ref(tab:tab-porig).

As secondary measures, we calculated the $p$-original values between a) the original and the rescue, b) the original and non-rescue replications, and c) the rescue and other replications.
For a) the rescue versus the original, median value of $p$-original was `r rescue_summ$median |> round(2)` [IQR: `r rescue_summ$low |> round(2)` - `r rescue_summ$high|> round(2)`], and `r round(rescue_summ$less_than*100)`% of the $p$-original values were less than .05.
For b) the non-rescue replications versus the original, median value of $p$-original was `r rep_summ$median |> round(2)` [IQR: `r rep_summ$low |> round(2)` - `r rep_summ$high |> round(2)`], and `r round(rep_summ$less_than*100)`% of the $p$-original values were less than .05.
For c) the rescue versus the other replications, median value of $p$-original was `r rescue_rep_summ$median |> round(2)` [IQR: `r rescue_rep_summ$low |> round(2)` - `r rescue_rep_summ$high |> round(2)`], and `r round(rescue_rep_summ$less_than*100)`% of the $p$-original values were less than .05.

<!--Visualizations of the statistical consistency between the original and each replication are shown in TODO FIGURE. The range around the open circle is the 95% predictive interval, representing the interval where, assuming that both are drawn from the same distribution, the replication will fall 95% of the time, given the precision (standard deviation) of the replication [@patil2016]. We again impute a heterogeneity value of $\tau=.21$ SMD for the construction of the prediction intervals.-->

<!--* We will use p-original to evaluate how consistent the original effect size is with the totality of replications. We expect there to be a small number of replications, so we will impute the heterogeneity value as in https://osf.io/preprints/psyarxiv/dpyn6/. -->

Overall, allowing for a heterogeneity level of $\tau$=.21 SMD, a number of rescues and replications are not statistically consistent with the original effects.
The pattern of inconsistency does not align with which studies were rated as having replicated.
In all cases, the point estimate of the re-replication is smaller than, or in the opposite direction of, the original effect on the key measure of interest.

```{r}
do_d_predInt_het <- function(df){
  if(!is.na(df$d)&!is.na(df$d_se)&!is.na(df$rep_d)&!is.na(df$rep_d_se)){
    yio=df$d
    vio=df$d_se**2
    yir=df$rep_d
    vir=df$rep_d_se**2
    t2=.21**2
    pooled.SE = sqrt(vio + vir + t2)
    PILo.sens = yio - qnorm(0.975) * pooled.SE
    PIHi.sens = yio + qnorm(0.975) * pooled.SE
    PIinside.sens = (yir > PILo.sens) & (yir < PIHi.sens)
    return(tibble(d_low=PILo.sens, d_high=PIHi.sens, d_inside=PIinside.sens))
  }
  return(tibble(d_low=NA,d_high=NA, d_inside=NA))
}

```

```{r}
orig <- parsed_d |> filter(type=="original") |> select(target_lastauthor_year, d=calc_d_calc, d_se=calc_d_calc_se, es=calc_ES, es_se=calc_SE)

predInts_het <- parsed_d |> 
  filter(type!="original") |> 
  group_by(target_lastauthor_year, name_pretty) |> 
  select(target_lastauthor_year, type, rep_d=calc_d_calc, rep_d_se=calc_d_calc_se, rep_es=calc_ES, rep_es_se=calc_SE) |> 
  left_join(orig) |> 
  group_by(target_lastauthor_year,type) |> 
  nest() |> 
  mutate(d_pred_int=map(data,do_d_predInt_het)) |> 
  unnest_wider(d_pred_int) |> 
  unnest_wider(data)


```

```{r, include=F,  fig.caption="Plot of the prediction intervals of the original studies compared to replication results. The open circles are SMD effect sizes from the original studies, error bars are the prediction intervals, and filled shapes denote replication effect sizes. Prediction intervals in green contain the replication effect, those in grey do not. The largest effect sizes are separated in a separate panel to allow better visualization. "}
colors <- c("original"="black", "rep1"="#377EB8", "rescue"="#E41A1C", "additional"="#984EA3")
shape <- c("original"=21,"rep1"=22,"rescue"=23,"additional"=24)
col_code <- c("TRUE"="darkgreen", "FALSE"="black")

big <- predInts_het |> filter(d>1) |> select(target_lastauthor_year)

small_predInt <-  ggplot(predInts_het |> filter(!is.na(d_inside)) |> anti_join(big), aes(x=reorder(name_pretty, d, FUN=max), y=rep_d, ymin=d_low, ymax=d_high, fill=type, color=d_inside, group=desc(type), shape=type))+
  geom_errorbar(size=.5, width=.25, position=position_dodge(width=.4), alpha=.5)+
  geom_point(aes(y=d), position=position_dodge(width=.4), shape=1, alpha=.5)+
     geom_point(size=2.5, position=position_dodge(width=.4), color="transparent")+
  coord_flip()+
  scale_size_area()+
  geom_hline(yintercept=0,color="black")+
  theme(legend.position="bottom", 
        panel.grid.major.x = element_blank(), 
        panel.grid.minor.x=element_blank()
        )+
  scale_fill_manual(values=colors)+
  scale_shape_manual(values=shape)+
  scale_color_manual(values=col_code)+
  labs(y="predInts on SMD scale", x="")

big_predInt <-  ggplot(predInts_het |> filter(!is.na(d_inside)) |> inner_join(big), aes(x=reorder(name_pretty, d, FUN=max), y=rep_d, ymin=d_low, ymax=d_high, color=d_inside, fill=type, group=desc(type), shape=type))+
  geom_errorbar(size=.5,width=.25, position=position_dodge(width=.4), alpha=.5)+
  geom_point(aes(y=d), position=position_dodge(width=.4), shape=1, alpha=.5)+
     geom_point(size=2.5, position=position_dodge(width=.4), color="transparent")+
  coord_flip()+
  scale_size_area()+
  geom_hline(yintercept=0,color="black")+
  theme(legend.position="none", 
        panel.grid.major.x = element_blank(), 
        panel.grid.minor.x=element_blank()
        )+
  scale_fill_manual(values=colors)+
  scale_shape_manual(values=shape)+
  scale_color_manual(values=col_code)+
  labs(y="predInts on SMD scale", x="")

plot_grid(big_predInt, small_predInt, nrow=2, rel_heights = c(.3,1))

```

# Discussion

We presented the results of `r num_proj` new replications that attempted to "rescue" previous failed replications reported in @boyce2023 by identifying and ameliorating possible causes of non-replication.
`r nrow(success)` of these re-replications (`r round(nrow(success)/num_proj*100)`%) mostly or fully replicated.

We don't have qualitative or quantitative explanations for why the specific studies did or did not replicate.
In some cases, increasing sample size and fixing internal validity issues in the replication seems to have led to a successful rescue (although we can't establish causality even in these cases).
However, there were other cases where the original replications had issues with a small sample or deviations in the implementation, and the re-replication addressed these issues but still failed to replicate the original results.
We can't predict what replication failures are likely to resolve given another, more thoughtful try, beyond that suggestion that glaring problems and low samples may sometimes be resolvable.

The re-replications all showed smaller effect sizes than their original studies, regardless of whether the pattern of effects replicated.
A large minority of replications had effect sizes that were statistically inconsistent with the original effect, even allowing for some heterogeneity between studies.
These diminished and inconsistent effects suggest that even if a re-replication "works", it may be difficult to build upon as follow-up studies will need large samples to detect small effects.

The reported rescue projects are a small sample of replications.
They are also chosen non-randomly, as they have been selected for twice by student interest.
However, this selection bias is likely to correlate with how graduate students choose what topics to work on and what studies to build on.
Nonetheless, with a small sample all our estimates are highly uncertain.

The authors of the rescue projects put substantial effort into trying to set up rescues that had a good chance of success, but projects were constrained by budget limitations, a short timeline, and primarily running online studies.
These limitations are representative of the sort of resource limitations often faced by early-career researchers.
That said, it is possible that different results might be obtained in better-resourced settings, by scientists with more expertise and more time.
Thus, we do not make statements about whether the original results are "true" or "false-positives", we merely claim they do not support cumulative research by early-career researchers under the constrained conditions we tested.

We opened this paper with a question about what an early-career psychology researcher should do given a failed replication: should they try again or move on?
From our sample of testing the "try again" approach, it seems that the odds of a re-replication working are low (consistent with @ebersole2020).
Especially if there is not a clear, identifiable reason for the first replication's failure, another try is unlikely to rescue the original result.

# Acknowledgements {.unnumbered}

# Author Contributions {.unnumbered}

# References {.unnumbered}

<!-- Use this magic to place references here. -->

::: {#refs}
:::
